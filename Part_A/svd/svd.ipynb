{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vSS0NhaDXdGp"
      },
      "outputs": [],
      "source": [
        "# import all necessary packages for CBOW\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import collections\n",
        "import itertools\n",
        "import re\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sJi0QAOuXdGr"
      },
      "outputs": [],
      "source": [
        "# # Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# # print device name: get_device_name()\n",
        "# print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EuZ6sx1yXdGr"
      },
      "outputs": [],
      "source": [
        "# Load data from file and store list of sentences where sentences are list of words\n",
        "class MakeSentences():\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.sentences = self.read_file()\n",
        "\n",
        "    def read_file(self):\n",
        "        sentences = []\n",
        "        with open(self.file_name, 'r') as f:\n",
        "            i=0\n",
        "            for line in f:\n",
        "                sentences += ([x for x in line.strip().split('.') if x!=''])\n",
        "                i+=1\n",
        "                if i==25000:\n",
        "                    break\n",
        "        return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n9lVgnqXdGr",
        "outputId": "c92fff35-1c9b-49b7-f9de-3a901ebdf386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61719\n"
          ]
        }
      ],
      "source": [
        "sentences = MakeSentences('./wikitext-2-raw-v1/wikitext-2-raw/wiki.train.raw').sentences\n",
        "print(len(sentences))\n",
        "# for sentence in sentences:\n",
        "#     print(type(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeHSAS2xXdGs"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vjibBu0XdGs"
      },
      "outputs": [],
      "source": [
        "class Preprocess():\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.sentences = [word_tokenize(sentence) for sentence in self.sentences]\n",
        "\n",
        "    def lowercase(self):\n",
        "        self.sentences = [[word.lower() for word in sentence] for sentence in self.sentences]\n",
        "\n",
        "    def remove_stop_words(self):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        self.sentences = [[word for word in sentence if word not in stop_words] for sentence in self.sentences]\n",
        "\n",
        "    def stemmer(self):\n",
        "        stemmer = nltk.stem.PorterStemmer()\n",
        "        self.sentences = [[stemmer.stem(word) for word in sentence] for sentence in self.sentences]\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.sentences = [[word for word in sentence if word.isalpha()] for sentence in self.sentences]\n",
        "\n",
        "    def remove_numbers(self):\n",
        "        self.sentences = [[word for word in sentence if not word.isdigit()] for sentence in self.sentences]\n",
        "\n",
        "    def remove_single_letter(self):\n",
        "        self.sentences = [[word for word in sentence if len(word) > 1] for sentence in self.sentences]\n",
        "\n",
        "    def remove_extra_spaces(self):\n",
        "        self.sentences = [[word for word in sentence if word != ' '] for sentence in self.sentences]\n",
        "\n",
        "    def remove_less_than_3(self):\n",
        "        self.sentences = [[word for word in sentence if len(word) > 2] for sentence in self.sentences]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJAu4MHXdGt",
        "outputId": "632db474-3bde-4af6-ef59-0b89eab5c7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing done\n",
            "61719\n"
          ]
        }
      ],
      "source": [
        "# preprocess\n",
        "preprocess = Preprocess(sentences)\n",
        "preprocess.tokenize()\n",
        "# print(preprocess.sentences)\n",
        "preprocess.lowercase()\n",
        "preprocess.remove_stop_words()\n",
        "# preprocess.stemmer()\n",
        "preprocess.remove_punctuation()\n",
        "preprocess.remove_numbers()\n",
        "preprocess.remove_single_letter()\n",
        "preprocess.remove_extra_spaces()\n",
        "preprocess.remove_less_than_3()\n",
        "\n",
        "print(\"Preprocessing done\")\n",
        "# print(preprocess.sentences)\n",
        "sentences = preprocess.sentences\n",
        "print(len(sentences))\n",
        "# print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj9DjLPeXdGt"
      },
      "source": [
        "### Create word index mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJy8gyPHXdGt",
        "outputId": "3a2ee884-0ce2-4576-ac4e-238e29d1aee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocab:  49155\n",
            "Most common words:  [('first', 2981), ('one', 2577), ('also', 2566), ('two', 2500), ('new', 1863), ('time', 1770), ('would', 1514), ('game', 1421), ('three', 1356), ('later', 1235)]\n"
          ]
        }
      ],
      "source": [
        "# Flatten list of sentences into list of words\n",
        "word_list = list(itertools.chain.from_iterable(sentences))\n",
        "# print(word_list)\n",
        "\n",
        "# Create a vocabulary of words\n",
        "word_freq = Counter(word_list)\n",
        "\n",
        "# Remove words that occur less than 5 times\n",
        "vocab = set(word if word_freq[word] > 0 else '<unk>' for word in word_list)\n",
        "# print(vocab)\n",
        "\n",
        "# Add padding and unknown token to vocab\n",
        "vocab.add('<pad>')\n",
        "vocab.add('<unk>')\n",
        "# Add start and end token to vocab\n",
        "vocab.add('<start>')\n",
        "vocab.add('<end>')\n",
        "\n",
        "# Print length of vocab\n",
        "print(\"Size of vocab: \", len(vocab))\n",
        "\n",
        "# Create word to index and index to word mapping\n",
        "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Print most common words\n",
        "print(\"Most common words: \", word_freq.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf6ZVtP7XdGt"
      },
      "outputs": [],
      "source": [
        "# create a co-oocurence matrix\n",
        "\n",
        "co_occurence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "window_size = 2\n",
        "\n",
        "for sentence in sentences:\n",
        "    # add start and end token to sentence\n",
        "    sentence = ['<start>'] + sentence + ['<end>']\n",
        "    for idx, word in enumerate(sentence):\n",
        "        if word in vocab:\n",
        "            # print(max(idx - window_size, 0), min(idx + window_size, len(sentence)) + 1)\n",
        "            for neighbor in sentence[max(idx - window_size, 0) : min(idx + window_size, len(sentence)) + 1]:\n",
        "                if neighbor != word and neighbor in vocab:\n",
        "                    # print(word, neighbor)\n",
        "                    co_occurence_matrix[word_to_idx[word]][word_to_idx[neighbor]] +=  1\n",
        "                    # print(co_occurence_matrix[word_to_idx[word]][word_to_idx[neighbor]])\n",
        "                    # co_occurence_matrix[word_to_idx[neighbor]][word_to_idx[word]] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOJQ4Q-dXdGu",
        "outputId": "e9c3069b-52f5-41b3-918f-9c86089eb012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.0\n",
            "(49155, 49155)\n"
          ]
        }
      ],
      "source": [
        "# Check co-occurence matrix\n",
        "print(co_occurence_matrix[word_to_idx['valkyria']][word_to_idx['iii']])\n",
        "print(co_occurence_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjGG_hsGXdGu",
        "outputId": "747b23c5-2a5b-47ec-863e-53547f2804c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nan\n",
            "Reduced dimension:  49155\n",
            "(49155, 49155)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "init_gesdd failed init\n"
          ]
        }
      ],
      "source": [
        "# Calculate embedding_dimension using SVD\n",
        "U, S, V = np.linalg.svd(co_occurence_matrix)\n",
        "\n",
        "# Check shape of U, S, V\n",
        "# print(U.shape)\n",
        "# print(S.shape)\n",
        "# print(V.shape)\n",
        "\n",
        "# Get variation to be 99%\n",
        "var = 0.90\n",
        "total_var = np.sum(S)\n",
        "print(total_var)\n",
        "var_sum = 0\n",
        "for i in range(len(S)):\n",
        "    var_sum += S[i]\n",
        "    if var_sum/total_var >= var:\n",
        "        break\n",
        "\n",
        "# Get reduced dimension\n",
        "dim = i + 1\n",
        "print(\"Reduced dimension: \", dim)\n",
        "\n",
        "# Get reduced U, S, V\n",
        "U = U[:, :dim]\n",
        "print(U.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "072RdZXOXdGu"
      },
      "outputs": [],
      "source": [
        "# Fit SVD\n",
        "embedding_dim = 300\n",
        "\n",
        "svd = TruncatedSVD(n_components=embedding_dim, n_iter=25, random_state=12, tol = 0.0, algorithm='arpack')\n",
        "final_matrix = svd.fit_transform(co_occurence_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKcMIQnOXdGu"
      },
      "outputs": [],
      "source": [
        "# Save final_matrix\n",
        "pickle.dump(final_matrix, open('./partA_pth/final_matrix.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd8ey_HXXdGu"
      },
      "outputs": [],
      "source": [
        "# Load final_matrix\n",
        "final_matrix = pickle.load(open('./partA_pth/final_matrix.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWJoXL6mXdGv"
      },
      "outputs": [],
      "source": [
        "# Write a function to get top k closest words to a given word\n",
        "def get_closest_k_words(word, k, final_matrix, word_to_idx, idx_to_word):\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_vector = final_matrix[word_idx]\n",
        "    similarity_scores = cosine_similarity([word_vector], final_matrix)\n",
        "    sorted_idxs = np.argsort(similarity_scores[0])[::-1]\n",
        "    closest_idxs = sorted_idxs[:k]\n",
        "    closest_words = [idx_to_word[idx] for idx in closest_idxs]\n",
        "    return closest_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxbxMTLwXdGv",
        "outputId": "478690b1-27aa-4f96-a1f4-3d1c295a9c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['woman', 'child', 'wife', 'marriage', 'baby', 'person', 'something', 'reala', 'gift', 'good']\n"
          ]
        }
      ],
      "source": [
        "# Check top 10 closest words to a given word\n",
        "print(get_closest_k_words('woman', 10, final_matrix, word_to_idx, idx_to_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAdD2KZPXdGv"
      },
      "source": [
        "### TSNE visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsg3MRICXdGv"
      },
      "outputs": [],
      "source": [
        "# Get the required indices\n",
        "word_list = ['woman', 'wife', 'film', 'sex', 'politics']\n",
        "\n",
        "all_words = set(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D2WA6bbXdGv"
      },
      "outputs": [],
      "source": [
        "# Function to print top k closest words to words in word_list\n",
        "def print_closest_k_words(word_list, k, final_matrix, word_to_idx, idx_to_word):\n",
        "    for word in word_list:\n",
        "        closest_words = get_closest_k_words(word, k, final_matrix, word_to_idx, idx_to_word)\n",
        "        print(\"Closest words to \", word, \": \", closest_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeJiQc-oXdGv",
        "outputId": "21bc68d6-58fa-4ca5-c466-80d4a9cac407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closest words to  woman :  ['woman', 'child', 'wife', 'marriage', 'baby', 'person', 'something', 'reala', 'gift', 'good']\n",
            "Closest words to  wife :  ['wife', 'marriage', 'child', 'mother', 'floor', 'baby', 'murder', 'nabucco', 'children', 'time']\n",
            "Closest words to  film :  ['film', 'novel', 'david', 'spisevognselskap', 'instead', 'writer', 'today', 'sanger', 'larry', 'media']\n",
            "Closest words to  sex :  ['sex', 'heart', 'religion', 'respectively', 'homosexuality', 'spp', 'kissing', 'loyalty', 'seamus', 'internet']\n",
            "Closest words to  politics :  ['politics', 'background', 'legacy', 'sbcl', 'mizuta', 'ariga', 'deceiving', 'lamaceratops', 'disproportionates', 'pyrolysis']\n"
          ]
        }
      ],
      "source": [
        "# Call print_closest_k_words\n",
        "print_closest_k_words(word_list, 10, final_matrix, word_to_idx, idx_to_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuHH29xQXdGv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
