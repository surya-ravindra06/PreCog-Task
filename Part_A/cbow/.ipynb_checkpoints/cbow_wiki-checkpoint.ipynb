{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/narayana_reedy/miniconda3/envs/vits/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages for CBOW\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import matutils\n",
    "from numpy import dot\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print device name: get_device_name()\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file and store list of sentences where sentences are list of words\n",
    "class MakeSentences():\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.sentences = self.read_file()\n",
    "\n",
    "    def read_file(self):\n",
    "        sentences = []\n",
    "        with open(self.file_name, 'r') as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                sentences += ([x for x in line.strip().split('.') if x!=''])\n",
    "                i+=1\n",
    "                if i==10000:\n",
    "                    break\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25137\n"
     ]
    }
   ],
   "source": [
    "sentences = MakeSentences('./wikitext-2-raw-v1/wikitext-2-raw/wiki.train.raw').sentences\n",
    "print(len(sentences))\n",
    "# for sentence in sentences:\n",
    "#     print(type(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.sentences = [word_tokenize(sentence) for sentence in self.sentences]\n",
    "\n",
    "    def lowercase(self):\n",
    "        self.sentences = [[word.lower() for word in sentence] for sentence in self.sentences]\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        self.sentences = [[word for word in sentence if word not in stop_words] for sentence in self.sentences]\n",
    "\n",
    "    def stemmer(self):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        self.sentences = [[stemmer.stem(word) for word in sentence] for sentence in self.sentences]\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        self.sentences = [[word for word in sentence if word.isalpha()] for sentence in self.sentences]\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        self.sentences = [[word for word in sentence if not word.isdigit()] for sentence in self.sentences]\n",
    "\n",
    "    def remove_single_letter(self):\n",
    "        self.sentences = [[word for word in sentence if len(word) > 1] for sentence in self.sentences]\n",
    "\n",
    "    def remove_extra_spaces(self):\n",
    "        self.sentences = [[word for word in sentence if word != ' '] for sentence in self.sentences]\n",
    "    \n",
    "    def remove_less_than_3(self):\n",
    "        self.sentences = [[word for word in sentence if len(word) > 2] for sentence in self.sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n",
      "25137\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "preprocess = Preprocess(sentences)\n",
    "preprocess.tokenize()\n",
    "# print(preprocess.sentences)\n",
    "preprocess.lowercase()\n",
    "preprocess.remove_stop_words()\n",
    "# preprocess.stemmer()\n",
    "preprocess.remove_punctuation()\n",
    "preprocess.remove_numbers()\n",
    "preprocess.remove_single_letter()\n",
    "preprocess.remove_extra_spaces()\n",
    "preprocess.remove_less_than_3()\n",
    "\n",
    "print(\"Preprocessing done\")\n",
    "# print(preprocess.sentences)\n",
    "sentences = preprocess.sentences\n",
    "print(len(sentences))\n",
    "# print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  30040\n",
      "Most common words:  [('also', 1056), ('one', 1045), ('first', 1019), ('two', 990), ('new', 774), ('time', 689), ('would', 601), ('three', 525), ('war', 507), ('may', 500)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten list of sentences into list of words\n",
    "word_list = list(itertools.chain.from_iterable(sentences))\n",
    "# print(word_list)\n",
    "\n",
    "# Create a vocabulary of words\n",
    "word_freq = Counter(word_list)\n",
    "\n",
    "# Remove words that occur less than 5 times\n",
    "vocab = set(word if word_freq[word] > 0 else '<unk>' for word in word_list)\n",
    "# print(vocab)\n",
    "\n",
    "# Add padding and unknown token to vocab\n",
    "vocab.add('<pad>')\n",
    "vocab.add('<unk>')\n",
    "# Add start and end token to vocab\n",
    "vocab.add('<start>')\n",
    "vocab.add('<end>')\n",
    "\n",
    "# Print length of vocab\n",
    "print(\"Size of vocab: \", len(vocab))\n",
    "\n",
    "# Create word to index and index to word mapping\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Print most common words\n",
    "print(\"Most common words: \", word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(word_to_idx['intelligent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "window_size = 2\n",
    "sliding_window = 2 * window_size + 1\n",
    "num_neg_samples = 5\n",
    "\n",
    "# sentences = [sentences[0]]\n",
    "# print(sentences)\n",
    "\n",
    "def get_samples(sentences, word_to_idx, idx_to_word, window_size, sliding_window, num_neg_sample):\n",
    "    X = []\n",
    "    y = [] \n",
    "    for sentence in sentences:\n",
    "        # add start and end token to sentence\n",
    "        sentence = ['<start>'] + sentence + ['<end>']\n",
    "        for i in range(len(sentence)):\n",
    "            target_word = sentence[i]\n",
    "            # print(\"target word: \", target_word)\n",
    "            context_words = []\n",
    "            temp1 = max(0,i - window_size)\n",
    "            temp2 = min(len(sentence)-1,i + window_size + 1)\n",
    "            # print(\"temp1: \", temp1)\n",
    "            # print(\"temp2: \", temp2)\n",
    "            for j in range(max(0,i - window_size),min(len(sentence)-1,i + window_size)+1):\n",
    "                if j != i:\n",
    "                    # print(sentence[j])\n",
    "                    context_words.append(sentence[j])\n",
    "                # print(\"context words: \", context_words)\n",
    "\n",
    "            \n",
    "            # pad context words if length is less than sliding window\n",
    "            if len(context_words) < sliding_window:\n",
    "                context_words += ['<pad>'] * (sliding_window - len(context_words)-1)\n",
    "            \n",
    "            context_words.append(target_word)\n",
    "            # print(\"length of context words: \", len(context_words))\n",
    "\n",
    "            # get positive samples \n",
    "            positive_samples = [word_to_idx[word] if word in vocab else word_to_idx['<unk>'] for word in context_words]\n",
    "            # print(\"lenght of positive samples: \", len(positive_samples))\n",
    "            \n",
    "            X.append(positive_samples)\n",
    "            y.append(1)\n",
    "\n",
    "\n",
    "            # get negative samples\n",
    "            negative_samples = []\n",
    "            while len(negative_samples) < num_neg_sample:\n",
    "                neg_sample = random.choice(list(vocab))\n",
    "                if neg_sample not in context_words:\n",
    "                    negative_samples += positive_samples[:-1]\n",
    "                    negative_samples += [word_to_idx[neg_sample]]\n",
    "                    X.append(negative_samples)\n",
    "                    y.append(0)\n",
    "    return X, y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating samples\n",
      "Samples created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating samples\")\n",
    "X = []\n",
    "y = []\n",
    "X,y = get_samples(sentences, word_to_idx, idx_to_word, window_size, sliding_window, num_neg_samples)\n",
    "print(\"Samples created\")\n",
    "# print(X)\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "data = list(zip(X,y))\n",
    "random.shuffle(data)\n",
    "X,y = zip(*data)\n",
    "\n",
    "# Turn into numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save X and y\n",
    "# np.save('./wiki_X_25k.npy', X)\n",
    "# np.save('./wiki_y_25k.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load X and y\n",
    "X = np.load('./datasets/wiki_X_25k.npy')\n",
    "y = np.load('./datasets/wiki_y_25k.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the forward pass for CBOW\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # self.embeddings.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # extract context and target from x\n",
    "        context = x[:, :-1]\n",
    "        target = x[:, -1]\n",
    "\n",
    "        context_embedding = self.embeddings(context)\n",
    "        context_embedding = torch.mean(context_embedding, dim=1)\n",
    "        target_embedding = self.embeddings(target)\n",
    "\n",
    "        # take dot product of context embedding and target embedding\n",
    "        score = torch.sum(context_embedding * target_embedding, dim=1)\n",
    "        return F.sigmoid(score)\n",
    "\n",
    "    def get_emebeddings(self):\n",
    "        out = self.embeddings.weight.data\n",
    "        return out.cpu().numpy()\n",
    "    \n",
    "    def get_word_emebdding(self, word):\n",
    "        # If word is not in vocab, return unk\n",
    "        if word not in word_to_idx:\n",
    "            word = '<unk>'\n",
    "        word_tensor = torch.LongTensor([word_to_idx[word]])\n",
    "        word_tensor = word_tensor.to(next(self.parameters()).device)\n",
    "        out = self.embeddings(word_tensor).data\n",
    "        return out.cpu().numpy()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/narayana_reedy/miniconda3/envs/vits/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Create model, loss function and optimizer\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "model.to(device)\n",
    "# Cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define dataloader\n",
    "dataset = torch.utils.data.TensorDataset(torch.from_numpy(X).long(), torch.from_numpy(y).float())\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19101, 16362, 17081,  7938, 17705],\n",
      "        [ 8876, 12157, 18589, 18589,    10],\n",
      "        [ 3318,  3765, 18312,  7228, 14759],\n",
      "        [ 3666,  5938, 19051, 19234, 20422],\n",
      "        [ 8876, 17293, 16648,   132, 16600],\n",
      "        [11105,  8320,  2459,  2459, 14542],\n",
      "        [ 9064, 11650,  7938, 15212,  1971],\n",
      "        [ 8839,  5637, 18589, 18589, 15917],\n",
      "        [17081,  2648,  3123, 11154, 14221],\n",
      "        [16142, 10225, 11925,  3358,  5583],\n",
      "        [ 8876,  5311, 20347, 18589,  8088],\n",
      "        [16764,  9129, 20440, 17775,  4956],\n",
      "        [ 5624, 11206, 12808, 19870, 21079],\n",
      "        [ 8876, 19627,  6243, 15619,   730],\n",
      "        [15178,  5102, 16362,  3110,  9895],\n",
      "        [ 6844,  4109, 14088, 20183,  3502],\n",
      "        [12706, 19627, 10048,   342,  9912],\n",
      "        [16427, 13928, 12157, 18589, 19319],\n",
      "        [12035, 12532,  5340, 20758, 14963],\n",
      "        [ 1281, 18459,  9239,  4354,  6387],\n",
      "        [ 7669,  9894, 18589, 18589,  3988],\n",
      "        [15992, 18976,  5053, 12157, 15918],\n",
      "        [11474, 14088, 18589, 18589, 17267],\n",
      "        [  646,  8107, 16490,  9310, 16787],\n",
      "        [ 8876,  1314,  1930, 17466,  4663],\n",
      "        [ 8876,  9753,  5872, 11421, 15155],\n",
      "        [ 6170,  7573,  7799, 17934, 13832],\n",
      "        [ 6425,   252,  7454,   132, 10108],\n",
      "        [ 8876,  9302, 20673,  2459, 18792],\n",
      "        [ 7978, 19336, 18589, 18589, 14380],\n",
      "        [ 8489,   623, 16425, 15261, 20996],\n",
      "        [ 8876, 12061, 17068, 21087,  8407],\n",
      "        [17838,  8937, 18589, 18589, 13147],\n",
      "        [18585,  9906,  3289,  5288, 13473],\n",
      "        [15651, 12902,  3998, 15694,  4697],\n",
      "        [ 2479, 18548, 17076,  8448,  5603],\n",
      "        [ 9652, 10509, 11274, 17076, 18540],\n",
      "        [ 7566,  7912,   181, 12157,   791],\n",
      "        [ 8213,  2883, 11496,  9193, 17332],\n",
      "        [ 8876,  8206, 18589, 18589, 12157],\n",
      "        [ 8320, 20080, 10415,  1867, 19557],\n",
      "        [10175,  4393,  7692,  6511,  7605],\n",
      "        [ 4248, 12927,  2361, 20068,  2627],\n",
      "        [10812,  5910, 11685,  4533, 19020],\n",
      "        [ 1015, 11601, 10580, 17827, 14485],\n",
      "        [ 8876,  6179,  5102,  5808, 20769],\n",
      "        [ 8971, 20661,  6565,   318, 15575],\n",
      "        [11351, 17933,  5828, 16530,  3495],\n",
      "        [ 2454,  9140, 15173,  7101, 17307],\n",
      "        [ 2372, 11538, 13124,  3538, 14184],\n",
      "        [  261, 17705, 13395, 12969,  5557],\n",
      "        [ 4492,  6509, 18589, 18589,  3109],\n",
      "        [11685,  1906,  1314,   729,  4267],\n",
      "        [ 8876, 10206, 18387,  3986,  5970],\n",
      "        [ 8876, 20705,   464, 19627,  6451],\n",
      "        [17423,  7560, 18921,  6609, 17519],\n",
      "        [15516,  6711, 13479, 15893, 19200],\n",
      "        [ 8876,  7669,  4625, 18589, 12807],\n",
      "        [ 7828, 10816,  9118, 17423,  3813],\n",
      "        [ 5987,  3931,  9144,  4003,  4144],\n",
      "        [ 8876, 12157, 18589, 18589,  8481],\n",
      "        [ 8876, 11587,  1314, 18589,  3714],\n",
      "        [ 6231, 16258, 12699, 12157, 10110],\n",
      "        [13846, 16258, 19233, 12684,  1957],\n",
      "        [10891,   148, 11580,  6870, 10891],\n",
      "        [ 2671, 19579, 17747,  3101,  3756],\n",
      "        [ 4781, 20669,  5897,  6335,  8489],\n",
      "        [20218,  6231,  9486, 16605, 14868],\n",
      "        [ 4987, 19050,   807,   575,  8794],\n",
      "        [15937, 10206,  3127,  1504, 11428],\n",
      "        [  514, 11636,  2819,  7976, 16115],\n",
      "        [15893, 11148,  1628,  9502,  7237],\n",
      "        [10470,  7178, 18589, 18589,  7620],\n",
      "        [16142,  8448, 18589, 18589,  8876],\n",
      "        [13479, 19020, 12157, 18589,  3627],\n",
      "        [19324,  1303, 11506,  8849,  8532],\n",
      "        [ 1801, 20271, 19472,  9843,  4662],\n",
      "        [ 8876, 16158, 15539,  7332,  6826],\n",
      "        [ 8876,  7227,  1068, 10523,  8384],\n",
      "        [15133,  9261, 12547, 12157, 20251],\n",
      "        [14795, 17076, 18589, 18589,  4654],\n",
      "        [ 5637,  4947,  6944,  1633, 11291],\n",
      "        [ 8876, 17076, 18271, 18589, 10657],\n",
      "        [11497,  7405,  5179,  5045, 15997],\n",
      "        [ 8473, 17343,  4193, 12085, 11102],\n",
      "        [ 7669, 12580, 18589, 18589, 16913],\n",
      "        [ 1939, 18578, 12036,  3768,  4699],\n",
      "        [ 3785,  8711,  4218, 12523,   966],\n",
      "        [16815,  1366,  9118, 16894, 14069],\n",
      "        [18690, 13174, 12157, 18589, 18645],\n",
      "        [ 1202,  3739,  2567,  8381,  2531],\n",
      "        [ 2118, 13500, 14531, 14555, 12752],\n",
      "        [21039,  1869,  3015, 20470, 19752],\n",
      "        [ 8263, 17309, 16214,  8136, 17022],\n",
      "        [ 8876, 16258, 11213, 18589, 16402],\n",
      "        [ 7268,  4150, 12318,  2647,  9167],\n",
      "        [18429,  4855,  4855, 18429, 18429],\n",
      "        [ 3285, 11474, 18422, 15726,  5193],\n",
      "        [16984, 10643,  2158, 12157,  7796],\n",
      "        [ 2507,  5763, 18589, 18589,  1546],\n",
      "        [14028,  9347, 19477,  9226,  5368],\n",
      "        [15694, 11319,  6664, 18429, 18299],\n",
      "        [ 2753,  4767, 17511,  2412,  7399],\n",
      "        [11907, 17423,  3567, 16530,  9675],\n",
      "        [15694, 11636,  8160, 19817,  5703],\n",
      "        [15201, 12759, 18382, 16142, 18128],\n",
      "        [15592, 12149, 16650, 20624,  4893],\n",
      "        [ 8876, 12157, 18589, 18589, 20481],\n",
      "        [ 1024, 19240,  7251, 17747, 20999],\n",
      "        [13729, 17081, 18589, 18589,  8876],\n",
      "        [ 8876, 15813, 10130, 18589, 12841],\n",
      "        [ 3966,  6389,  8343, 14788, 20500],\n",
      "        [16362,  8367, 20440,  1504, 20392],\n",
      "        [ 8876,  6092, 12380, 18589, 13464],\n",
      "        [ 8876, 20403,  5850,  5562, 15236],\n",
      "        [17685, 16700, 18589, 18589, 14327],\n",
      "        [  235, 19627,  7978, 15261, 14778],\n",
      "        [ 5102, 20679,  8308,  4979, 20225],\n",
      "        [12157, 18589, 18589, 18589,  2261],\n",
      "        [ 8876,  2085,  2918, 18589,  6872],\n",
      "        [ 1670,  1314,   230, 20188,  9497],\n",
      "        [13593,  5102, 12157, 18589,  8265],\n",
      "        [ 8950, 17423, 14925, 18271, 20188],\n",
      "        [ 7692,  1314,   933, 10331,  7877],\n",
      "        [ 9278, 15033,  2012, 17525, 18436],\n",
      "        [10716, 12360, 11503, 10403,  6813],\n",
      "        [ 3255, 15625,   344, 12157, 10575],\n",
      "        [11496, 10620,  6597,  1517, 10698]])\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    print(inputs)\n",
    "    print(targets)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "def train(model, criterion, optimizer, dataloader, epochs):\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        labels = []\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # get predictions\n",
    "            preds = [1 if x > 0.5 else 0 for x in outputs]\n",
    "            train_preds.extend(preds)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            labels.extend(targets)\n",
    "        \n",
    "        train_loss /= len(dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Epoch: \", epoch+1, \"Loss: \", train_loss)\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  1.705165450592815\n",
      "Epoch:  2 Loss:  1.0486615673932522\n",
      "Epoch:  3 Loss:  0.6510741380387365\n",
      "Epoch:  4 Loss:  0.41271156435144307\n",
      "Epoch:  5 Loss:  0.27073331407031953\n",
      "Epoch:  6 Loss:  0.18297851787226876\n",
      "Epoch:  7 Loss:  0.12701677682263918\n",
      "Epoch:  8 Loss:  0.08982207287339403\n",
      "Epoch:  9 Loss:  0.06427968618276836\n",
      "Epoch:  10 Loss:  0.04632108771202467\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_losses = train(model, criterion, optimizer, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train accuracy\n",
    "def plot_train_losses(train_losses):\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title('Train Loss vs Epochs')\n",
    "    plt.savefig('./plots/cbow_train_losses.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call function to plot train accuracy\n",
    "plot_train_losses(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# torch.save(model.state_dict(), './partA_pth/cbow_25k_100_001.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(30040, 100)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "model.load_state_dict(torch.load('./partA_pth/cbow_25k_100_001.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with Simlex-999 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
      "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
      "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
      "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
      "3  happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
      "4   hard         easy   A       0.95      3.76      2.07      2        5.82   \n",
      "\n",
      "   SimAssoc333  SD(SimLex)  \n",
      "0            1        0.41  \n",
      "1            1        0.67  \n",
      "2            1        1.19  \n",
      "3            1        2.18  \n",
      "4            1        0.93  \n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "\n",
    "# Load into dataframe\n",
    "df = pd.read_csv('./SimLex-999/SimLex-999.txt', sep='\\t')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart\n"
     ]
    }
   ],
   "source": [
    "# Print 2nd row word1\n",
    "print(df['word1'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "(100,)\n",
      "[ 0.18075277 -0.9057839   0.40108305 -0.9070025  -0.6975531   0.31167555\n",
      " -0.35879743 -0.36171755 -0.8349153   0.07477821 -0.80363595  0.53388727\n",
      "  1.8571227   0.22264518  0.961409    0.77008975 -1.0178255   0.55819386\n",
      "  1.7245463  -0.7068674  -0.8024187   0.81649125  0.40620068  0.25892806\n",
      "  1.5255837   2.0803037  -0.21916707  0.36171588 -0.70025533 -1.1892884\n",
      "  0.42310715  0.31445092 -0.8234355  -1.5093013  -1.476535   -0.6414965\n",
      " -1.1422263   0.25345257  0.23207897 -0.23928949 -0.38756707 -1.0637864\n",
      " -0.03699321 -1.4359326   0.6845426   0.9630441  -0.7452512   1.3193133\n",
      "  0.77783716  0.16029109  0.53764176 -0.5237524  -1.6386174  -0.5409553\n",
      "  0.7688113   0.8073961  -1.8344827  -1.6788822  -1.8443093  -0.08770707\n",
      "  1.0734423   0.7910537   0.06175035 -2.1622736   1.4035774   1.048304\n",
      "  0.12439346  0.7854449   1.5961334   0.32623097  0.30411124  0.61046\n",
      " -0.20056142  0.53998065  0.7704427   1.0840112   0.39525625  1.01482\n",
      " -0.72130316 -0.64964074 -1.174599   -0.1652472  -1.0125525   2.1409872\n",
      "  0.52739143  1.3993474   0.7194586  -1.4397174   0.9258396   0.5538536\n",
      " -1.9647287   0.60855556  0.49316207 -0.2949288   1.3675134  -0.6994739\n",
      "  0.37788695 -1.3233002   0.12999739 -0.03004249]\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "# Get word embeddings\n",
    "sample_embedding = model.get_word_emebdding(df['word1'][1])\n",
    "print(sample_embedding.shape)\n",
    "sample_embedding = sample_embedding.squeeze()\n",
    "print(sample_embedding.shape)\n",
    "print(sample_embedding)\n",
    "sample_embedding = sample_embedding.reshape(1,-1)\n",
    "print(sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check similarity between two words\n",
    "word1 = df['word1'][1]\n",
    "word2 = df['word1'][1]\n",
    "# Use gensim matutils to calculate cosine similarity\n",
    "w1 = model.get_word_emebdding(word1).squeeze()\n",
    "w2 = model.get_word_emebdding(word2).squeeze()\n",
    "print(type(w1))\n",
    "\n",
    "sim = dot(matutils.unitvec(w1), matutils.unitvec(w2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if word is in vocab\n",
    "def check_vocab(word):\n",
    "    if word in vocab:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get cosine similarity\n",
    "def cos_similarity(word1_embedding, word2_embedding):\n",
    "    ans = dot(matutils.unitvec(word1_embedding), matutils.unitvec(word2_embedding))\n",
    "    return ans\n",
    "\n",
    "# Function to get Pearson correlation\n",
    "def pearson_correlation(word1_embedding, word2_embedding):\n",
    "    emb1 = np.array(word1_embedding)\n",
    "    emb2 = np.array(word2_embedding)\n",
    "\n",
    "    correlation, _ = pearsonr(emb1, emb2)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim(df, model, lemmatizer, stemmer):\n",
    "    cosine_similarity_scores = []\n",
    "    pearson_correlation_scores = []\n",
    "    simlex_scores = []\n",
    "    not_in_vocab = 0\n",
    "  \n",
    "    for _, row in df.iterrows():\n",
    "        word1 = row['word1']\n",
    "        word2 = row['word2']\n",
    "        form = row['POS']\n",
    "        form = form.lower()\n",
    "       \n",
    "        # Check if word is in vocab\n",
    "        if not check_vocab(word1) or not check_vocab(word2):\n",
    "            not_in_vocab += 1\n",
    "\n",
    "        # Get embeddings\n",
    "        word1_embedding = model.get_word_emebdding(word1).squeeze()\n",
    "        word2_embedding = model.get_word_emebdding(word2).squeeze()\n",
    "\n",
    "        # Get cosine similarity\n",
    "        cosine_similarity_scores.append(cos_similarity(word1_embedding, word2_embedding))\n",
    "        \n",
    "        # Get pearson correlation\n",
    "        pearson_correlation_scores.append(pearson_correlation(word1_embedding, word2_embedding))\n",
    "\n",
    "        # Get simlex score\n",
    "        simlex_scores.append(row['SimLex999'])\n",
    "        \n",
    "    return cosine_similarity_scores, pearson_correlation_scores, simlex_scores, not_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity and pearson correlation scores\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "cosine_similarity_scores, pearson_correlation_scores, simlex_scores, not_in_vocab = test_sim(df, model, lemmatizer, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# Check cosine similarity and pearson correlation scores\n",
    "print(type(cosine_similarity_scores))\n",
    "print(type(pearson_correlation_scores))\n",
    "print(type(simlex_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtcion to get spearman correlation using cosine similarity scores\n",
    "def spearman_correlation(cosine_similarity_scores, simlex_scores):\n",
    "    # Scale cosine similarity scores to 0-10\n",
    "    cosine_similarity_scores = np.array(cosine_similarity_scores)\n",
    "    cosine_similarity_scores = (1+cosine_similarity_scores)*5\n",
    "    simlex_scores = np.array(simlex_scores)\n",
    "\n",
    "    correlation, _ = spearmanr(cosine_similarity_scores, simlex_scores)\n",
    "    return correlation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Spearman correlation Sim:  0.002999494524414288\n"
     ]
    }
   ],
   "source": [
    "# Print the initial spearman correlation\n",
    "spearman_value_sim = spearman_correlation(cosine_similarity_scores, simlex_scores)\n",
    "\n",
    "print(\"Initial Spearman correlation Sim: \", spearman_value_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points not in vocab:  128\n"
     ]
    }
   ],
   "source": [
    "# Print the number of data points not in vocab\n",
    "print(\"Number of data points not in vocab: \", not_in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1        word2 POS  SimLex999  Assoc(USF)  Cosine Similarity  \\\n",
      "0    old          new   A       1.58        7.25          -0.051120   \n",
      "1  smart  intelligent   A       9.20        7.11           0.067963   \n",
      "2   hard    difficult   A       8.77        5.94           0.004693   \n",
      "3  happy     cheerful   A       9.55        5.85          -0.053577   \n",
      "4   hard         easy   A       0.95        5.82           0.109989   \n",
      "\n",
      "   Pearson Correlation  \n",
      "0            -0.059328  \n",
      "1             0.067981  \n",
      "2             0.012462  \n",
      "3            -0.069699  \n",
      "4             0.107372  \n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe of cosine similarity scores and pearson correlation scores along with Simlex-999 scores and Assoc(USF)\n",
    "simlex_scores = df['SimLex999']\n",
    "assoc_scores = df['Assoc(USF)']\n",
    "cosine_similarity_scores = np.array(cosine_similarity_scores)\n",
    "pearson_correlation_scores = np.array(pearson_correlation_scores)\n",
    "simlex_scores = np.array(simlex_scores)\n",
    "assoc_scores = np.array(assoc_scores)\n",
    "# print(cosine_similarity_scores.shape)\n",
    "# print(pearson_correlation_scores.shape)\n",
    "\n",
    "# Make a dataframe along with word1, word2, POS, SimLex-999 scores, Assoc(USF), cosine similarity scores and pearson correlation scores\n",
    "datat = {'word1': df['word1'], 'word2': df['word2'], 'POS': df['POS'], 'SimLex999': simlex_scores, 'Assoc(USF)': assoc_scores, 'Cosine Similarity': cosine_similarity_scores, 'Pearson Correlation': pearson_correlation_scores}\n",
    "ndf = pd.DataFrame(data=datat)\n",
    "print(ndf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word1        word2 POS  SimLex999  Assoc(USF)  Cosine Similarity  \\\n",
      "0       old          new   A       1.58        7.25          -0.051120   \n",
      "1     smart  intelligent   A       9.20        7.11           0.067963   \n",
      "2      hard    difficult   A       8.77        5.94           0.004693   \n",
      "3     happy     cheerful   A       9.55        5.85          -0.053577   \n",
      "4      hard         easy   A       0.95        5.82           0.109989   \n",
      "..      ...          ...  ..        ...         ...                ...   \n",
      "994    join      acquire   V       2.85        0.00          -0.033160   \n",
      "995    send       attend   V       1.67        0.00          -0.072337   \n",
      "996  gather       attend   V       4.80        0.00          -0.127210   \n",
      "997  absorb     withdraw   V       2.97        0.00          -0.085305   \n",
      "998  attend       arrive   V       6.08        0.00           0.073885   \n",
      "\n",
      "     Pearson Correlation  \n",
      "0              -0.059328  \n",
      "1               0.067981  \n",
      "2               0.012462  \n",
      "3              -0.069699  \n",
      "4               0.107372  \n",
      "..                   ...  \n",
      "994            -0.033081  \n",
      "995            -0.052997  \n",
      "996            -0.117106  \n",
      "997            -0.071758  \n",
      "998             0.091530  \n",
      "\n",
      "[999 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print df\n",
    "print(ndf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "def create_dataset(df, model):\n",
    "    # Create a list of tuples\n",
    "    emb1 = []\n",
    "    emb2 = []\n",
    "    simlex_scores = []\n",
    "    assoc_scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word1 = row['word1']\n",
    "        word2 = row['word2']\n",
    "        emb1.append(torch.tensor(model.get_word_emebdding(word1).squeeze()))\n",
    "        emb2.append(torch.tensor(model.get_word_emebdding(word2).squeeze()))\n",
    "        simlex_scores.append(row['SimLex999'])\n",
    "        assoc_scores.append(row['Assoc(USF)'])\n",
    "    \n",
    "    # print(emb1[0].shape)\n",
    "    emb1_stack = torch.stack(emb1)\n",
    "    emb2_stack = torch.stack(emb2)\n",
    "    \n",
    "    return emb1_stack, emb2_stack, torch.tensor(simlex_scores, dtype=torch.float), torch.tensor(assoc_scores, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_dataset\n",
    "train_df, test_df = train_test_split(ndf, test_size=0.1, random_state=42)\n",
    "train_emb1, train_emb2, train_simlex_scores, train_assoc_scores = create_dataset(train_df, model)\n",
    "test_emb1, test_emb2, test_simlex_scores, test_assoc_scores = create_dataset(test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([899, 100])\n",
      "torch.Size([899])\n"
     ]
    }
   ],
   "source": [
    "# check train_emb1\n",
    "print(train_emb1.shape)\n",
    "print(train_simlex_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creare TensorDataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_emb1, train_emb2, train_simlex_scores, train_assoc_scores)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_emb1, test_emb2, test_simlex_scores, test_assoc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that takes CBOW embeddings, and outputs similarity scores: loss is MSE between predicted similarity scores and actual similarity scores(Simlex-999)\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(2*embedding_dim, 50)\n",
    "        self.linear2 = nn.Linear(50, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, emb1, emb2):\n",
    "        # emb1 = emb1.squeeze()\n",
    "        # emb2 = emb2.squeeze()\n",
    "        emb = torch.cat((emb1, emb2), dim=1)\n",
    "\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        # Project the output between 0 and 10\n",
    "        out = torch.sigmoid(out)\n",
    "        out = out*10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model\n",
    "lmodel = RegressionModel(embedding_dim).to(device)\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(lmodel.parameters(), lr=learning_rate, weight_decay=0.01) # weight_decay is L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        for emb1, emb2, simlex_scores, assoc_scores in train_loader:\n",
    "            emb1 = emb1.to(device)\n",
    "            emb2 = emb2.to(device)\n",
    "            simlex_scores = simlex_scores.to(device)\n",
    "            assoc_scores = assoc_scores.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(emb1, emb2)\n",
    "            \n",
    "            simlex_scores = simlex_scores.unsqueeze(1)\n",
    "            # print(outputs[0])\n",
    "            loss = criterion(outputs, simlex_scores)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        print(\"Epoch: {}, Train_Loss: {}\".format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 7.52900559326698\n",
      "Epoch: 2, Train_Loss: 6.631259675896267\n",
      "Epoch: 3, Train_Loss: 6.362641539932534\n",
      "Epoch: 4, Train_Loss: 5.675068591064463\n",
      "Epoch: 5, Train_Loss: 5.76198632874798\n",
      "Epoch: 6, Train_Loss: 5.186327361292781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 4.920491872958314\n",
      "Epoch: 8, Train_Loss: 4.687337947259414\n",
      "Epoch: 9, Train_Loss: 4.1514107137017575\n",
      "Epoch: 10, Train_Loss: 4.180713929489745\n",
      "Epoch: 11, Train_Loss: 3.689290043874564\n",
      "Epoch: 12, Train_Loss: 3.8589865913735033\n",
      "Epoch: 13, Train_Loss: 3.5997646586331356\n",
      "Epoch: 14, Train_Loss: 3.284228481562085\n",
      "Epoch: 15, Train_Loss: 3.317683336300108\n",
      "Epoch: 16, Train_Loss: 3.130790952881261\n",
      "Epoch: 17, Train_Loss: 2.7834415067215104\n",
      "Epoch: 18, Train_Loss: 2.8247872660522466\n",
      "Epoch: 19, Train_Loss: 2.741235379754692\n",
      "Epoch: 20, Train_Loss: 2.6488287244849915\n",
      "Epoch: 21, Train_Loss: 2.492385214069155\n",
      "Epoch: 22, Train_Loss: 2.648233189853398\n",
      "Epoch: 23, Train_Loss: 2.3124855268505926\n",
      "Epoch: 24, Train_Loss: 2.367368616278513\n",
      "Epoch: 25, Train_Loss: 2.1919186909176682\n",
      "Epoch: 26, Train_Loss: 2.2051126408822843\n",
      "Epoch: 27, Train_Loss: 2.1353537565272687\n",
      "Epoch: 28, Train_Loss: 2.0020780300470657\n",
      "Epoch: 29, Train_Loss: 2.010805753919491\n",
      "Epoch: 30, Train_Loss: 2.036588303935014\n",
      "Epoch: 31, Train_Loss: 2.0479984075410433\n",
      "Epoch: 32, Train_Loss: 1.882771487176361\n",
      "Epoch: 33, Train_Loss: 1.809224648555803\n",
      "Epoch: 34, Train_Loss: 1.8165612400016802\n",
      "Epoch: 35, Train_Loss: 1.8702574727879226\n",
      "Epoch: 36, Train_Loss: 1.9670443649879348\n",
      "Epoch: 37, Train_Loss: 1.7860038407773506\n",
      "Epoch: 38, Train_Loss: 1.9569755303727168\n",
      "Epoch: 39, Train_Loss: 1.896908382259109\n",
      "Epoch: 40, Train_Loss: 1.8227458013776494\n",
      "Epoch: 41, Train_Loss: 1.8111406606287914\n",
      "Epoch: 42, Train_Loss: 1.7564659931942348\n",
      "Epoch: 43, Train_Loss: 1.771502448288478\n",
      "Epoch: 44, Train_Loss: 1.6796428894574573\n",
      "Epoch: 45, Train_Loss: 1.7051975553596674\n",
      "Epoch: 46, Train_Loss: 1.6629143474614643\n",
      "Epoch: 47, Train_Loss: 1.6147895870473057\n",
      "Epoch: 48, Train_Loss: 1.6120325279573806\n",
      "Epoch: 49, Train_Loss: 1.5647276720519046\n",
      "Epoch: 50, Train_Loss: 1.5579355464622726\n",
      "Epoch: 51, Train_Loss: 1.6301794405170056\n",
      "Epoch: 52, Train_Loss: 1.5017010643406428\n",
      "Epoch: 53, Train_Loss: 1.5585969997354066\n",
      "Epoch: 54, Train_Loss: 1.515714175786464\n",
      "Epoch: 55, Train_Loss: 1.4969684936289775\n",
      "Epoch: 56, Train_Loss: 1.3930877573280662\n",
      "Epoch: 57, Train_Loss: 1.5677651705652447\n",
      "Epoch: 58, Train_Loss: 1.326419502299099\n",
      "Epoch: 59, Train_Loss: 1.4878003443041434\n",
      "Epoch: 60, Train_Loss: 1.3546357611978157\n",
      "Epoch: 61, Train_Loss: 1.3714093016487567\n",
      "Epoch: 62, Train_Loss: 1.3703495019020095\n",
      "Epoch: 63, Train_Loss: 1.3244343391552105\n",
      "Epoch: 64, Train_Loss: 1.4251124344731698\n",
      "Epoch: 65, Train_Loss: 1.5001623571688598\n",
      "Epoch: 66, Train_Loss: 1.3407782637893972\n",
      "Epoch: 67, Train_Loss: 1.3144880174351143\n",
      "Epoch: 68, Train_Loss: 1.2556107585500451\n",
      "Epoch: 69, Train_Loss: 1.4085287244322342\n",
      "Epoch: 70, Train_Loss: 1.3140480202888702\n",
      "Epoch: 71, Train_Loss: 1.2506862066342066\n",
      "Epoch: 72, Train_Loss: 1.3184803634613191\n",
      "Epoch: 73, Train_Loss: 1.2744672541979039\n",
      "Epoch: 74, Train_Loss: 1.3538245361603645\n",
      "Epoch: 75, Train_Loss: 1.224388269862763\n",
      "Epoch: 76, Train_Loss: 1.2385597845014906\n",
      "Epoch: 77, Train_Loss: 1.4305635855804126\n",
      "Epoch: 78, Train_Loss: 1.4079438823605757\n",
      "Epoch: 79, Train_Loss: 1.3750801654319147\n",
      "Epoch: 80, Train_Loss: 1.264741961201238\n",
      "Epoch: 81, Train_Loss: 1.2837666027861152\n",
      "Epoch: 82, Train_Loss: 1.2318623571324532\n",
      "Epoch: 83, Train_Loss: 1.2414832214813405\n",
      "Epoch: 84, Train_Loss: 1.1730461160009653\n",
      "Epoch: 85, Train_Loss: 1.2171437577258084\n",
      "Epoch: 86, Train_Loss: 1.3044495597510863\n",
      "Epoch: 87, Train_Loss: 1.1689830464022917\n",
      "Epoch: 88, Train_Loss: 1.091180158216027\n",
      "Epoch: 89, Train_Loss: 1.2320641365735787\n",
      "Epoch: 90, Train_Loss: 1.3265463630601217\n",
      "Epoch: 91, Train_Loss: 1.2930061148380905\n",
      "Epoch: 92, Train_Loss: 1.1789361420951208\n",
      "Epoch: 93, Train_Loss: 1.2369318610761424\n",
      "Epoch: 94, Train_Loss: 1.2703593167982357\n",
      "Epoch: 95, Train_Loss: 1.2016235685667835\n",
      "Epoch: 96, Train_Loss: 1.2478290100766376\n",
      "Epoch: 97, Train_Loss: 1.1250697062614086\n",
      "Epoch: 98, Train_Loss: 1.2388042087739997\n",
      "Epoch: 99, Train_Loss: 1.3040652435243345\n",
      "Epoch: 100, Train_Loss: 1.2172588166876093\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train(lmodel, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model, Calculate test loss and Spearman correlation\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    true_simlex_scores = []\n",
    "    pred_simlex_scores = []\n",
    "    for emb1, emb2, simlex_scores, assoc_scores in test_loader:\n",
    "        emb1 = emb1.to(device)\n",
    "        emb2 = emb2.to(device)\n",
    "        simlex_scores = simlex_scores.to(device)\n",
    "        assoc_scores = assoc_scores.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(emb1, emb2)\n",
    "        simlex_scores = simlex_scores.unsqueeze(1)\n",
    "        loss = criterion(outputs, simlex_scores)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get true labels and predicted labels\n",
    "        true_simlex_scores.extend(simlex_scores.cpu().detach().numpy().tolist())\n",
    "        pred_simlex_scores.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(\"Test_Loss: {}\".format(test_loss))\n",
    "    # Calculate Spearman correlation\n",
    "    # print(\"True Simlex scores: \", true_simlex_scores)\n",
    "    # print(\"Predicted Simlex scores: \", pred_simlex_scores)\n",
    "\n",
    "    true_simlex_scores = np.array(true_simlex_scores)\n",
    "    pred_simlex_scores = np.array(pred_simlex_scores)\n",
    "    spear = spearmanr(true_simlex_scores, pred_simlex_scores)\n",
    "    print(\"Spearman correlation: {}\".format(spear[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Loss: 9.290817856788635\n",
      "Spearman correlation: -0.06560111860538273\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test(lmodel, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
