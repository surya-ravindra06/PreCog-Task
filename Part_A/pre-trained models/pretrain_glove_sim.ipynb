{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages for CBOW\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import matutils\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print device name: get_device_name()\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 300])\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B')\n",
    "print(glove.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between king and queen:  5.966258525848389\n",
      "Cosine similarity between king and queen:  0.6336469650268555\n",
      "New Distance between king and queen:  4.753939628601074\n",
      "New Cosine similarity between king and queen:  0.8065859079360962\n"
     ]
    }
   ],
   "source": [
    "# Sample check\n",
    "x = glove.vectors[glove.stoi['king']]\n",
    "y = glove.vectors[glove.stoi['queen']]\n",
    "# z = king - man + woman\n",
    "z = x - glove.vectors[glove.stoi['man']] + glove.vectors[glove.stoi['woman']]\n",
    "print(\"Distance between king and queen: \", torch.norm(x - y).item())\n",
    "print(\"Cosine similarity between king and queen: \", F.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)).item())\n",
    "print(\"New Distance between king and queen: \", torch.norm(x - z).item())\n",
    "print(\"New Cosine similarity between king and queen: \", F.cosine_similarity(x.unsqueeze(0), z.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance for Short vs Long: tensor(4.4622)\n",
      "Cosine similarity for Short vs Long: tensor([0.6962])\n",
      "Distance for Smart vs Intelligent: tensor(5.0731)\n",
      "Cosine similarity for Smart vs Intelligent: tensor([0.6520])\n"
     ]
    }
   ],
   "source": [
    "# Check glove\n",
    "# print(glove.vectors[glove.stoi['long']])\n",
    "x = glove.vectors[glove.stoi['short']]\n",
    "y = glove.vectors[glove.stoi['long']]\n",
    "print(\"Distance for Short vs Long:\", torch.norm(x - y))\n",
    "print(\"Cosine similarity for Short vs Long:\",torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)))\n",
    "\n",
    "x = glove.vectors[glove.stoi['smart']]\n",
    "y = glove.vectors[glove.stoi['intelligent']]\n",
    "print(\"Distance for Smart vs Intelligent:\", torch.norm(x - y))\n",
    "print(\"Cosine similarity for Smart vs Intelligent:\",torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return glove embedding of a word\n",
    "def get_word_embedding(word):\n",
    "    return glove.vectors[glove.stoi[word] if word in glove.stoi else glove.stoi['unk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
      "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
      "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
      "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
      "3  happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
      "4   hard         easy   A       0.95      3.76      2.07      2        5.82   \n",
      "\n",
      "   SimAssoc333  SD(SimLex)  \n",
      "0            1        0.41  \n",
      "1            1        0.67  \n",
      "2            1        1.19  \n",
      "3            1        2.18  \n",
      "4            1        0.93  \n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "\n",
    "# Load into dataframe\n",
    "df = pd.read_csv('./SimLex-999/SimLex-999.txt', sep='\\t')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "tensor([ 0.0785, -0.1332, -0.0080, -0.6235,  0.4710,  0.4107,  0.4210,  0.0975,\n",
      "        -0.0953, -1.5386,  0.7887, -0.1068,  0.4298,  0.0091,  0.2274,  0.4928,\n",
      "        -0.2278, -0.2156,  0.5345, -0.0212,  0.5884,  0.6723,  0.1577,  0.2165,\n",
      "         0.0198, -0.1534,  0.0908,  0.4564,  0.4809,  0.1248, -0.2552,  0.4312,\n",
      "        -0.4340,  0.5759, -0.9858,  0.6472, -0.1101, -0.1227,  0.3032, -0.1306,\n",
      "        -0.0108,  0.1437, -0.0154,  0.2334, -0.0767, -0.4027,  0.1049, -0.4329,\n",
      "         0.0226, -0.3252, -0.0767,  0.3358, -0.0463, -0.0706, -0.1404, -0.1653,\n",
      "        -0.3133,  0.2234,  0.0640, -0.2646, -0.0804, -0.0948, -0.2048, -0.0995,\n",
      "         0.0983,  0.5361,  0.0348,  0.3095, -0.4390, -0.1759, -0.2365, -0.2690,\n",
      "         0.4018,  0.2489,  0.0699,  0.1214, -0.0119,  0.1006,  0.4084, -0.5842,\n",
      "        -0.1472, -0.2617,  0.4712,  0.0417, -0.0841,  0.0027, -0.5950,  0.1395,\n",
      "         0.0989, -0.1532, -0.3652,  0.6814,  0.0942, -0.3264,  0.6242, -0.1298,\n",
      "        -0.3847,  0.0694,  0.4051, -0.6522, -0.5461, -0.2016,  0.0655, -0.0813,\n",
      "        -0.1783, -0.3032,  0.2702,  0.2438, -0.0856, -0.6193,  0.2062, -0.0704,\n",
      "         0.2832,  0.2850,  0.0721,  0.3259,  0.2196, -0.4081,  0.1454,  0.1562,\n",
      "        -0.1574,  0.0507,  0.2099, -0.4314, -0.4616, -0.1117,  0.3578,  0.2673,\n",
      "         0.2866, -0.4823,  0.5343, -0.2814, -0.5463,  0.3849, -0.3940, -0.1437,\n",
      "         0.0081, -0.3205, -0.2234,  0.0055,  0.1356,  0.2059,  0.4930, -0.1081,\n",
      "        -0.4218,  0.5304, -0.3860, -0.3413, -0.3036,  0.2764, -0.3739, -0.0235,\n",
      "         0.2144,  0.1335,  0.1021,  0.0029, -0.3594, -0.2601, -0.0905, -0.0875,\n",
      "        -0.3981,  0.1594,  0.4023, -0.2914, -0.2664, -0.4316, -0.0609,  0.2383,\n",
      "         0.2396, -0.1060,  0.4278,  0.0745, -0.8793,  0.4144, -0.0968, -0.3456,\n",
      "        -0.5131, -0.1917, -0.2124, -0.1889,  0.2967, -0.7444,  0.6315,  0.0193,\n",
      "         0.2615, -0.1134,  0.4303, -0.1125,  0.0129, -0.2966, -0.1826, -0.0266,\n",
      "         0.3556,  0.2741, -0.4211,  0.0220, -0.7768, -0.0348,  0.1703, -0.2153,\n",
      "         0.8630,  0.0281,  0.1948,  0.1730, -0.1908, -0.2516,  0.1877,  0.1445,\n",
      "         0.2422, -0.0941,  0.3094, -0.3176,  0.0897,  0.1383, -0.2689, -0.2053,\n",
      "         0.3629,  0.2230,  0.0518, -0.2742, -0.5030,  0.4612, -0.0893,  0.5777,\n",
      "         0.1878,  0.5682,  0.2446, -0.4637,  0.0470, -0.0822, -0.1249,  0.0570,\n",
      "        -0.0377,  0.0455, -0.4564,  0.2924, -0.1216,  0.0818, -0.4664,  0.6265,\n",
      "         0.6528,  0.0765, -0.3616,  0.1818, -0.2512, -0.3595, -0.4886, -0.4071,\n",
      "         0.4032,  0.0869,  0.4255, -0.4944, -0.7160,  0.3161,  0.0181, -0.1819,\n",
      "         0.8725,  0.2667,  0.1813,  0.0769, -0.1306, -0.1425, -0.4475,  0.4814,\n",
      "         0.4135,  0.0123,  0.0908, -0.4656,  0.0726,  0.4368,  0.1368,  0.1097,\n",
      "        -0.3709, -0.6249, -0.0145,  0.3574, -1.0558,  0.6858, -0.6719, -0.4207,\n",
      "        -0.2169,  0.0940,  0.2524, -0.2409, -0.3334,  0.1934,  0.3256, -0.2096,\n",
      "        -0.7283, -0.3550,  0.2746,  0.2140, -0.2312,  0.1263,  0.0685, -0.0658,\n",
      "         0.1432, -0.3204,  0.3566,  0.1335])\n"
     ]
    }
   ],
   "source": [
    "# Get word embeddings\n",
    "sample_embedding = get_word_embedding(df['word1'][1])\n",
    "print(sample_embedding.shape)\n",
    "sample_embedding = sample_embedding.squeeze()\n",
    "print(sample_embedding.shape)\n",
    "print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check similarity between two words\n",
    "word1 = df['word1'][1]\n",
    "word2 = df['word1'][1]\n",
    "# Use gensim matutils to calculate cosine similarity\n",
    "w1 = get_word_embedding(word1)\n",
    "w2 = get_word_embedding(word2)\n",
    "# Convert to numpy array\n",
    "w1 = w1.numpy()\n",
    "w2 = w2.numpy()\n",
    "print(type(w1))\n",
    "\n",
    "sim = dot(matutils.unitvec(w1), matutils.unitvec(w2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.58719975\n"
     ]
    }
   ],
   "source": [
    "# Check similarity between two words\n",
    "word1 = 'big'\n",
    "word2 = 'large'\n",
    "# Use gensim matutils to calculate cosine similarity\n",
    "w1 = get_word_embedding(word1)\n",
    "w2 = get_word_embedding(word2)\n",
    "# Convert to numpy array\n",
    "w1 = w1.numpy()\n",
    "w2 = w2.numpy()\n",
    "print(type(w1))\n",
    "\n",
    "sim = dot(matutils.unitvec(w1), matutils.unitvec(w2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get cosine similarity\n",
    "def cos_similarity(word1_embedding, word2_embedding):\n",
    "    word1_embedding = np.array(word1_embedding)\n",
    "    word2_embedding = np.array(word2_embedding)\n",
    "   \n",
    "    ans = dot(matutils.unitvec(word1_embedding), matutils.unitvec(word2_embedding))\n",
    "    return ans\n",
    "\n",
    "# Function to get Pearson correlation\n",
    "def pearson_correlation(word1_embedding, word2_embedding):\n",
    "    emb1 = np.array(word1_embedding)\n",
    "    emb2 = np.array(word2_embedding)\n",
    "\n",
    "    correlation, _ = pearsonr(emb1, emb2)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim(df, lemmatizer, stemmer):\n",
    "    cosine_similarity_scores = []\n",
    "    pearson_correlation_scores = []\n",
    "    simlex_scores = []\n",
    "    assoc_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        word1 = row['word1']\n",
    "        word2 = row['word2']\n",
    "        form = row['POS']\n",
    "        form = form.lower()\n",
    "        \n",
    "        word1_embedding = get_word_embedding(word1).squeeze()\n",
    "        word2_embedding = get_word_embedding(word2).squeeze()\n",
    "\n",
    "        # Get cosine similarity\n",
    "        cosine_similarity_scores.append(cos_similarity(word1_embedding, word2_embedding))\n",
    "        \n",
    "        # Get pearson correlation\n",
    "        pearson_correlation_scores.append(pearson_correlation(word1_embedding, word2_embedding))\n",
    "\n",
    "        # Get simlex score\n",
    "        simlex_scores.append(row['SimLex999'])\n",
    "\n",
    "        # Get assoc score\n",
    "        assoc_scores.append(row['Assoc(USF)'])\n",
    "        \n",
    "    return cosine_similarity_scores, pearson_correlation_scores, simlex_scores, assoc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity and pearson correlation scores\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "cosine_similarity_scores, pearson_correlation_scores, simlex_scores, assoc_scores = test_sim(df, lemmatizer, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Check cosine similarity and pearson correlation scores\n",
    "print(type(cosine_similarity_scores))\n",
    "print(type(pearson_correlation_scores))\n",
    "print(type(simlex_scores))\n",
    "print(type(assoc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtcion to get spearman correlation using cosine similarity scores\n",
    "def spearman_correlation(cosine_similarity_scores, simlex_scores):\n",
    "    # Scale cosine similarity scores to 0-10\n",
    "    cosine_similarity_scores = np.array(cosine_similarity_scores)\n",
    "    cosine_similarity_scores = (1+cosine_similarity_scores)*5\n",
    "    simlex_scores = np.array(simlex_scores)\n",
    "\n",
    "    correlation, _ = spearmanr(cosine_similarity_scores, simlex_scores)\n",
    "    return correlation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Spearman correlation Sim:  0.37050035710869067\n",
      "Initial Spearman correlation Assoc:  0.38807153931964644\n"
     ]
    }
   ],
   "source": [
    "# Print the initial spearman correlation\n",
    "spearman_value_sim = spearman_correlation(cosine_similarity_scores, simlex_scores)\n",
    "spearman_value_assoc = spearman_correlation(cosine_similarity_scores, assoc_scores)\n",
    "print(\"Initial Spearman correlation Sim: \", spearman_value_sim)\n",
    "print(\"Initial Spearman correlation Assoc: \", spearman_value_assoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1        word2 POS  SimLex999  Assoc(USF)  Cosine Similarity  \\\n",
      "0    old          new   A       1.58        7.25           0.367693   \n",
      "1  smart  intelligent   A       9.20        7.11           0.652035   \n",
      "2   hard    difficult   A       8.77        5.94           0.635781   \n",
      "3  happy     cheerful   A       9.55        5.85           0.440317   \n",
      "4   hard         easy   A       0.95        5.82           0.578426   \n",
      "\n",
      "   Pearson Correlation  \n",
      "0             0.368144  \n",
      "1             0.652400  \n",
      "2             0.636245  \n",
      "3             0.441698  \n",
      "4             0.579278  \n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe of cosine similarity scores and pearson correlation scores along with Simlex-999 scores and Assoc(USF)\n",
    "simlex_scores = df['SimLex999']\n",
    "assoc_scores = df['Assoc(USF)']\n",
    "cosine_similarity_scores = np.array(cosine_similarity_scores)\n",
    "pearson_correlation_scores = np.array(pearson_correlation_scores)\n",
    "simlex_scores = np.array(simlex_scores)\n",
    "assoc_scores = np.array(assoc_scores)\n",
    "# print(cosine_similarity_scores.shape)\n",
    "# print(pearson_correlation_scores.shape)\n",
    "\n",
    "# Make a dataframe along with word1, word2, POS, SimLex-999 scores, Assoc(USF), cosine similarity scores and pearson correlation scores\n",
    "datat = {'word1': df['word1'], 'word2': df['word2'], 'POS': df['POS'], 'SimLex999': simlex_scores, 'Assoc(USF)': assoc_scores, 'Cosine Similarity': cosine_similarity_scores, 'Pearson Correlation': pearson_correlation_scores}\n",
    "ndf = pd.DataFrame(data=datat)\n",
    "print(ndf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word1        word2 POS  SimLex999  Assoc(USF)  Cosine Similarity  \\\n",
      "0       old          new   A       1.58        7.25           0.367693   \n",
      "1     smart  intelligent   A       9.20        7.11           0.652035   \n",
      "2      hard    difficult   A       8.77        5.94           0.635781   \n",
      "3     happy     cheerful   A       9.55        5.85           0.440317   \n",
      "4      hard         easy   A       0.95        5.82           0.578426   \n",
      "..      ...          ...  ..        ...         ...                ...   \n",
      "994    join      acquire   V       2.85        0.00           0.285346   \n",
      "995    send       attend   V       1.67        0.00           0.341219   \n",
      "996  gather       attend   V       4.80        0.00           0.406434   \n",
      "997  absorb     withdraw   V       2.97        0.00           0.155071   \n",
      "998  attend       arrive   V       6.08        0.00           0.441711   \n",
      "\n",
      "     Pearson Correlation  \n",
      "0               0.368144  \n",
      "1               0.652400  \n",
      "2               0.636245  \n",
      "3               0.441698  \n",
      "4               0.579278  \n",
      "..                   ...  \n",
      "994             0.288044  \n",
      "995             0.341869  \n",
      "996             0.406458  \n",
      "997             0.154866  \n",
      "998             0.442133  \n",
      "\n",
      "[999 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print df\n",
    "print(ndf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "def create_dataset(df):\n",
    "    # Create a list of tuples\n",
    "    emb1 = []\n",
    "    emb2 = []\n",
    "    simlex_scores = []\n",
    "    assoc_scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word1 = row['word1']\n",
    "        word2 = row['word2']\n",
    "        emb1.append(get_word_embedding(word1))\n",
    "        emb2.append(get_word_embedding(word2))\n",
    "        simlex_scores.append(row['SimLex999'])\n",
    "        assoc_scores.append(row['Assoc(USF)'])\n",
    "    \n",
    "    # print(emb1[0].shape)\n",
    "    emb1_stack = torch.stack(emb1)\n",
    "    emb2_stack = torch.stack(emb2)\n",
    "    \n",
    "    return emb1_stack, emb2_stack, torch.tensor(simlex_scores, dtype=torch.float), torch.tensor(assoc_scores, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_dataset\n",
    "train_df, test_df = train_test_split(ndf, test_size=0.1, random_state=42)\n",
    "train_emb1, train_emb2, train_simlex_scores, train_assoc_scores = create_dataset(train_df)\n",
    "test_emb1, test_emb2, test_simlex_scores, test_assoc_scores = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([899, 300])\n",
      "torch.Size([899])\n"
     ]
    }
   ],
   "source": [
    "# check train_emb1\n",
    "print(train_emb1.shape)\n",
    "print(train_simlex_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creare TensorDataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_emb1, train_emb2, train_simlex_scores, train_assoc_scores)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_emb1, test_emb2, test_simlex_scores, test_assoc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that takes CBOW embeddings, and outputs similarity scores: loss is MSE between predicted similarity scores and actual similarity scores(Simlex-999)\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(2*embedding_dim, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 50)\n",
    "        self.linear4 = nn.Linear(50, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, emb1, emb2):\n",
    "        # emb1 = emb1.squeeze()\n",
    "        # emb2 = emb2.squeeze()\n",
    "        emb = torch.cat((emb1, emb2), dim=1)\n",
    "\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear4(out)\n",
    "\n",
    "        # Project the output between 0 and 10\n",
    "        out = torch.sigmoid(out)\n",
    "        out = out*10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embedding_dim = 300\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model\n",
    "model = RegressionModel(embedding_dim).to(device)\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01) # weight_decay is L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        for emb1, emb2, simlex_scores, assoc_scores in train_loader:\n",
    "            emb1 = emb1.to(device)\n",
    "            emb2 = emb2.to(device)\n",
    "            simlex_scores = simlex_scores.to(device)\n",
    "            assoc_scores = assoc_scores.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(emb1, emb2)\n",
    "            \n",
    "            simlex_scores = simlex_scores.unsqueeze(1)\n",
    "            # print(outputs[0])\n",
    "            loss = criterion(outputs, simlex_scores)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        print(\"Epoch: {}, Train_Loss: {}\".format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 6.82242201114523\n",
      "Epoch: 2, Train_Loss: 7.082542566850551\n",
      "Epoch: 3, Train_Loss: 6.688761798417252\n",
      "Epoch: 4, Train_Loss: 5.538591963391047\n",
      "Epoch: 5, Train_Loss: 4.565945429443646\n",
      "Epoch: 6, Train_Loss: 3.805899424211709\n",
      "Epoch: 7, Train_Loss: 3.7285589611001577\n",
      "Epoch: 8, Train_Loss: 2.816798715877354\n",
      "Epoch: 9, Train_Loss: 2.2790207222819268\n",
      "Epoch: 10, Train_Loss: 2.0375593467543864\n",
      "Epoch: 11, Train_Loss: 2.0096794762967316\n",
      "Epoch: 12, Train_Loss: 1.7291252462300704\n",
      "Epoch: 13, Train_Loss: 1.5648862323823778\n",
      "Epoch: 14, Train_Loss: 1.4902625769091173\n",
      "Epoch: 15, Train_Loss: 1.3206737023993214\n",
      "Epoch: 16, Train_Loss: 1.134516274046637\n",
      "Epoch: 17, Train_Loss: 1.0681309568166644\n",
      "Epoch: 18, Train_Loss: 1.1408377942932058\n",
      "Epoch: 19, Train_Loss: 1.1782147783571701\n",
      "Epoch: 20, Train_Loss: 1.2133882268356668\n",
      "Epoch: 21, Train_Loss: 1.0943431512517745\n",
      "Epoch: 22, Train_Loss: 1.0167855413343818\n",
      "Epoch: 23, Train_Loss: 0.9439069693512832\n",
      "Epoch: 24, Train_Loss: 0.731580727025788\n",
      "Epoch: 25, Train_Loss: 0.7973892778749069\n",
      "Epoch: 26, Train_Loss: 0.7550170434980379\n",
      "Epoch: 27, Train_Loss: 0.7677568156605095\n",
      "Epoch: 28, Train_Loss: 0.7832250840331121\n",
      "Epoch: 29, Train_Loss: 0.7715563299562226\n",
      "Epoch: 30, Train_Loss: 0.7258161171835924\n",
      "Epoch: 31, Train_Loss: 0.747830140229485\n",
      "Epoch: 32, Train_Loss: 0.7320209202970726\n",
      "Epoch: 33, Train_Loss: 0.6988470987951814\n",
      "Epoch: 34, Train_Loss: 0.6740188181964695\n",
      "Epoch: 35, Train_Loss: 0.6790372088057149\n",
      "Epoch: 36, Train_Loss: 0.8560811783202159\n",
      "Epoch: 37, Train_Loss: 0.7638759342600823\n",
      "Epoch: 38, Train_Loss: 0.7286764404820639\n",
      "Epoch: 39, Train_Loss: 0.7106706685774875\n",
      "Epoch: 40, Train_Loss: 0.6495939674286778\n",
      "Epoch: 41, Train_Loss: 0.6379589804366702\n",
      "Epoch: 42, Train_Loss: 0.8012706670554051\n",
      "Epoch: 43, Train_Loss: 0.8121068211581226\n",
      "Epoch: 44, Train_Loss: 0.6957276451765324\n",
      "Epoch: 45, Train_Loss: 0.6318798907466964\n",
      "Epoch: 46, Train_Loss: 0.6167880777924507\n",
      "Epoch: 47, Train_Loss: 0.7147390273261661\n",
      "Epoch: 48, Train_Loss: 0.6227101047390103\n",
      "Epoch: 49, Train_Loss: 0.7471520393215151\n",
      "Epoch: 50, Train_Loss: 0.6420263209189577\n",
      "Epoch: 51, Train_Loss: 0.6889786581823387\n",
      "Epoch: 52, Train_Loss: 0.718111832116627\n",
      "Epoch: 53, Train_Loss: 0.7101524884733175\n",
      "Epoch: 54, Train_Loss: 0.8330907134497051\n",
      "Epoch: 55, Train_Loss: 0.7745403505546057\n",
      "Epoch: 56, Train_Loss: 1.0803949057274242\n",
      "Epoch: 57, Train_Loss: 0.9893999967054841\n",
      "Epoch: 58, Train_Loss: 1.0050227600030388\n",
      "Epoch: 59, Train_Loss: 0.7648578469701617\n",
      "Epoch: 60, Train_Loss: 0.740568074291972\n",
      "Epoch: 61, Train_Loss: 0.5799281523558609\n",
      "Epoch: 62, Train_Loss: 0.6463537778865781\n",
      "Epoch: 63, Train_Loss: 0.704192555041188\n",
      "Epoch: 64, Train_Loss: 0.7237243094352153\n",
      "Epoch: 65, Train_Loss: 0.797656388249573\n",
      "Epoch: 66, Train_Loss: 0.7017545659366207\n",
      "Epoch: 67, Train_Loss: 0.7645943502551624\n",
      "Epoch: 68, Train_Loss: 0.7581029874838848\n",
      "Epoch: 69, Train_Loss: 0.7288306231051939\n",
      "Epoch: 70, Train_Loss: 1.0955126949245568\n",
      "Epoch: 71, Train_Loss: 1.109026282231338\n",
      "Epoch: 72, Train_Loss: 0.9722347798772354\n",
      "Epoch: 73, Train_Loss: 0.8689429788850768\n",
      "Epoch: 74, Train_Loss: 0.7556556587557102\n",
      "Epoch: 75, Train_Loss: 0.7378472361247433\n",
      "Epoch: 76, Train_Loss: 0.711548506608231\n",
      "Epoch: 77, Train_Loss: 0.642261200916857\n",
      "Epoch: 78, Train_Loss: 0.6186836955798114\n",
      "Epoch: 79, Train_Loss: 0.6422363016038531\n",
      "Epoch: 80, Train_Loss: 0.7377922355718632\n",
      "Epoch: 81, Train_Loss: 0.6336526073244358\n",
      "Epoch: 82, Train_Loss: 0.7251460380842876\n",
      "Epoch: 83, Train_Loss: 0.6438563737632577\n",
      "Epoch: 84, Train_Loss: 0.6062512725706818\n",
      "Epoch: 85, Train_Loss: 0.6344967177205932\n",
      "Epoch: 86, Train_Loss: 0.7991986952660196\n",
      "Epoch: 87, Train_Loss: 0.7552036409488962\n",
      "Epoch: 88, Train_Loss: 0.6789010796221907\n",
      "Epoch: 89, Train_Loss: 0.6643251140681932\n",
      "Epoch: 90, Train_Loss: 0.6294312927906816\n",
      "Epoch: 91, Train_Loss: 0.60265185556804\n",
      "Epoch: 92, Train_Loss: 0.6522797764450637\n",
      "Epoch: 93, Train_Loss: 0.5851916068624845\n",
      "Epoch: 94, Train_Loss: 0.6307228567060088\n",
      "Epoch: 95, Train_Loss: 0.5942464642547199\n",
      "Epoch: 96, Train_Loss: 0.5879098714211896\n",
      "Epoch: 97, Train_Loss: 0.5801110789721701\n",
      "Epoch: 98, Train_Loss: 0.6317503912200376\n",
      "Epoch: 99, Train_Loss: 0.5472735806715552\n",
      "Epoch: 100, Train_Loss: 0.5494893490220352\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model, Calculate test loss and Spearman correlation\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    true_simlex_scores = []\n",
    "    pred_simlex_scores = []\n",
    "    for emb1, emb2, simlex_scores, assoc_scores in test_loader:\n",
    "        emb1 = emb1.to(device)\n",
    "        emb2 = emb2.to(device)\n",
    "        simlex_scores = simlex_scores.to(device)\n",
    "        assoc_scores = assoc_scores.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(emb1, emb2)\n",
    "        simlex_scores = simlex_scores.unsqueeze(1)\n",
    "        loss = criterion(outputs, simlex_scores)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get true labels and predicted labels\n",
    "        true_simlex_scores.extend(simlex_scores.cpu().detach().numpy().tolist())\n",
    "        pred_simlex_scores.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(\"Test_Loss: {}\".format(test_loss))\n",
    "    # Calculate Spearman correlation\n",
    "    # print(\"True Simlex scores: \", true_simlex_scores)\n",
    "    # print(\"Predicted Simlex scores: \", pred_simlex_scores)\n",
    "\n",
    "    true_simlex_scores = np.array(true_simlex_scores)\n",
    "    pred_simlex_scores = np.array(pred_simlex_scores)\n",
    "    spear = spearmanr(true_simlex_scores, pred_simlex_scores)\n",
    "    print(\"Spearman correlation: {}\".format(spear[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Loss: 8.3381667137146\n",
      "Spearman correlation: 0.17455226289730819\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
