{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages for CBOW\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import matutils\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print device name: get_device_name()\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 300])\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B')\n",
    "print(glove.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between king and queen:  5.966258525848389\n",
      "Cosine similarity between king and queen:  0.6336469650268555\n",
      "New Distance between king and queen:  4.753939628601074\n",
      "New Cosine similarity between king and queen:  0.8065859079360962\n"
     ]
    }
   ],
   "source": [
    "# Sample check\n",
    "x = glove.vectors[glove.stoi['king']]\n",
    "y = glove.vectors[glove.stoi['queen']]\n",
    "# z = king - man + woman\n",
    "z = x - glove.vectors[glove.stoi['man']] + glove.vectors[glove.stoi['woman']]\n",
    "print(\"Distance between king and queen: \", torch.norm(x - y).item())\n",
    "print(\"Cosine similarity between king and queen: \", F.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)).item())\n",
    "print(\"New Distance between king and queen: \", torch.norm(x - z).item())\n",
    "print(\"New Cosine similarity between king and queen: \", F.cosine_similarity(x.unsqueeze(0), z.unsqueeze(0)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance for Short vs Long: tensor(4.4622)\n",
      "Cosine similarity for Short vs Long: tensor([0.6962])\n",
      "Distance for Smart vs Intelligent: tensor(5.0731)\n",
      "Cosine similarity for Smart vs Intelligent: tensor([0.6520])\n"
     ]
    }
   ],
   "source": [
    "# Check glove\n",
    "# print(glove.vectors[glove.stoi['long']])\n",
    "x = glove.vectors[glove.stoi['short']]\n",
    "y = glove.vectors[glove.stoi['long']]\n",
    "print(\"Distance for Short vs Long:\", torch.norm(x - y))\n",
    "print(\"Cosine similarity for Short vs Long:\",torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)))\n",
    "\n",
    "x = glove.vectors[glove.stoi['smart']]\n",
    "y = glove.vectors[glove.stoi['intelligent']]\n",
    "print(\"Distance for Smart vs Intelligent:\", torch.norm(x - y))\n",
    "print(\"Cosine similarity for Smart vs Intelligent:\",torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return glove embedding of a word\n",
    "def get_word_embedding(word):\n",
    "    return glove.vectors[glove.stoi[word] if word in glove.stoi else glove.stoi['unk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word 1    Word 2  Human (mean)\n",
      "0      love       sex          6.77\n",
      "1     tiger       cat          7.35\n",
      "2     tiger     tiger         10.00\n",
      "3      book     paper          7.46\n",
      "4  computer  keyboard          7.62\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "\n",
    "# Load into dataframe\n",
    "df = pd.read_csv('./wordsim353/combined.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "tensor([ 3.1805e-01,  3.8612e-01,  1.0725e-01,  2.8261e-01, -4.4965e-02,\n",
      "         1.0612e-02,  4.3426e-01,  1.1006e+00,  1.5124e-01, -7.5199e-01,\n",
      "         5.4254e-01, -2.5544e-01, -1.6400e-01,  1.6128e-01, -1.7060e-02,\n",
      "        -2.2410e-01,  1.2682e-01,  8.4087e-01, -2.7631e-01,  4.4310e-02,\n",
      "         2.6123e-01, -3.8948e-02, -1.4925e-01, -6.0481e-01, -1.1059e+00,\n",
      "        -1.1135e-01, -5.9403e-02, -2.2909e-01,  6.7889e-01,  1.8288e-01,\n",
      "         6.9610e-02, -1.3831e+00,  5.7360e-02, -3.3441e-01, -2.6577e-01,\n",
      "        -3.4069e-01,  1.7086e-01,  5.9148e-01, -8.3631e-01,  4.8743e-01,\n",
      "         2.4388e-01, -4.2785e-01,  3.9639e-01, -1.8224e-01, -3.1574e-01,\n",
      "        -4.1929e-01,  4.3294e-01, -3.1500e-01, -2.3390e-01, -9.5833e-03,\n",
      "         9.6671e-01, -1.8473e-01,  1.5179e-01,  3.5956e-01, -5.4430e-02,\n",
      "         2.4032e-01, -1.7691e-02,  1.0346e+00, -2.3621e-01, -4.6284e-02,\n",
      "        -6.3183e-01, -2.6131e-01,  2.2495e-01,  6.5933e-01,  9.7632e-02,\n",
      "        -1.4428e-01, -5.1098e-01, -6.4340e-01,  2.2279e-01,  4.7017e-01,\n",
      "         6.8450e-02,  3.4013e-01,  5.0337e-02,  2.4793e-02, -1.3726e-02,\n",
      "         2.2475e-01,  5.3832e-01, -6.0123e-01,  2.4434e-01,  3.5062e-01,\n",
      "         1.4058e-01, -2.4493e-01,  1.0419e-01, -4.9484e-01,  3.2629e-01,\n",
      "        -6.3158e-01, -8.1345e-01, -3.5166e-02,  2.3366e-01, -2.6914e-01,\n",
      "        -1.5012e-02, -9.1551e-02, -3.5173e-01,  3.2239e-02,  4.4592e-01,\n",
      "         3.2620e-01,  2.0522e-01, -4.6460e-01, -5.6885e-01, -4.6610e-01,\n",
      "         9.7700e-02,  3.7100e-01,  5.5914e-01,  3.3662e-01,  5.5955e-01,\n",
      "        -2.6679e-02,  2.0628e-02,  1.6219e-01, -7.9194e-02,  2.1179e-01,\n",
      "        -8.1665e-02,  9.6164e-02, -8.2605e-01, -1.6286e-01,  9.8834e-02,\n",
      "         8.9302e-02,  1.8239e-01,  5.2664e-01, -6.4723e-01, -2.7549e-01,\n",
      "        -1.0490e+00, -5.9390e-01, -2.0139e-01,  5.8160e-01, -2.9544e-01,\n",
      "        -2.3005e-01,  1.9733e-01,  3.1993e-01,  4.0029e-02, -4.2565e-01,\n",
      "        -2.6076e-01,  2.8575e-01, -1.0009e-01, -2.0921e-02, -1.6854e-01,\n",
      "         2.3219e-01,  1.2139e-02, -1.6396e-01, -2.2856e-01,  3.1307e-01,\n",
      "         4.4448e-02,  2.7773e-02,  4.2594e-01, -4.2870e-01, -3.9471e-01,\n",
      "        -4.0541e-01,  6.3980e-02,  5.5482e-01,  4.8681e-02, -2.6031e-01,\n",
      "        -2.5607e-01, -6.8518e-02, -2.1721e-01, -2.1251e-01,  4.8762e-01,\n",
      "        -4.8040e-01, -3.9291e-01, -2.6542e-01,  3.0685e-01,  1.3879e+00,\n",
      "         3.1551e-01,  3.7906e-01,  4.8222e-02, -2.6885e-01, -6.7447e-01,\n",
      "        -5.8845e-02,  1.4894e-02,  3.1901e-01, -8.5330e-01, -5.3998e-01,\n",
      "        -6.5799e-01, -2.2534e-01, -9.9321e-02,  7.0429e-01,  1.1255e-01,\n",
      "        -2.8921e-01,  1.4851e-01,  2.5052e-01,  5.3332e-01, -1.3101e-02,\n",
      "         6.0388e-02, -1.5534e-01, -1.6244e-01,  1.8062e-01, -2.5835e-03,\n",
      "         1.0043e-01, -1.4026e-01,  2.9892e-01,  2.6348e-01, -7.3950e-02,\n",
      "         3.7324e-01,  1.5981e-01,  2.0407e-01,  1.0287e-01,  6.6057e-02,\n",
      "         9.6447e-02,  4.9990e-01, -3.2505e-02,  1.1403e-01, -3.0171e-01,\n",
      "         1.8904e+00, -4.2511e-01, -1.4158e-01, -5.4888e-01, -2.0008e-01,\n",
      "         3.7909e-01, -6.6070e-01, -2.0747e-01, -3.6918e-01,  6.8243e-02,\n",
      "         5.6493e-02,  6.7445e-02, -2.1361e-01, -9.9830e-01,  3.2986e-01,\n",
      "        -5.5691e-01,  1.7576e-01,  3.5422e-01, -6.5196e-02, -1.6417e-02,\n",
      "         8.3042e-01,  2.6428e-01,  2.9994e-01, -5.1640e-01,  1.2353e-01,\n",
      "        -4.6543e-01,  3.8272e-01, -3.6424e-01, -3.6278e-01, -5.6585e-01,\n",
      "         2.3366e-01, -7.2896e-01,  3.5874e-01, -6.2963e-03, -8.0878e-02,\n",
      "        -1.9360e-01,  2.1159e-01, -9.0342e-02, -7.9771e-01,  3.0855e-01,\n",
      "        -4.8318e-01,  1.3295e-01,  4.0856e-02,  9.5406e-01, -7.3737e-01,\n",
      "         4.6077e-01, -8.2662e-02,  2.2545e-01,  2.5722e-01, -5.7956e-01,\n",
      "        -1.1102e+00, -2.4182e-01, -5.3534e-01, -3.9995e-01,  8.9786e-01,\n",
      "        -4.7030e-01,  6.8895e-01, -6.4400e-02, -3.0525e-01, -2.4539e-01,\n",
      "         1.0649e-01,  1.1519e-04,  5.2123e-02, -2.3651e-01, -2.7918e-01,\n",
      "         1.2954e-01,  1.3222e-01,  4.5636e-01, -1.6590e-01, -1.6413e-01,\n",
      "         2.1242e-01, -9.8367e-02,  2.7643e-01, -3.5059e-01,  4.2767e-01,\n",
      "        -3.6123e-01, -5.3538e-01,  1.1485e-01, -2.5226e-01, -1.7993e-01,\n",
      "        -1.4363e-01,  1.2369e-01,  2.3747e-01,  2.0453e-01, -6.1958e-01,\n",
      "        -2.3841e-01, -5.7274e-01, -2.2474e-01,  2.1683e-01, -2.8839e-01,\n",
      "        -3.5495e-01, -4.4978e-01, -7.3920e-01, -1.0828e-01,  6.0953e-03,\n",
      "         9.4683e-01,  3.6775e-01,  1.4240e-01,  2.5970e-01,  2.5982e-01])\n"
     ]
    }
   ],
   "source": [
    "# Get word embeddings\n",
    "sample_embedding = get_word_embedding(df['Word 1'][1])\n",
    "print(sample_embedding.shape)\n",
    "sample_embedding = sample_embedding.squeeze()\n",
    "print(sample_embedding.shape)\n",
    "print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check similarity between two words\n",
    "word1 = df['Word 1'][1]\n",
    "word2 = df['Word 1'][1]\n",
    "# Use gensim matutils to calculate cosine similarity\n",
    "w1 = get_word_embedding(word1)\n",
    "w2 = get_word_embedding(word2)\n",
    "# Convert to numpy array\n",
    "w1 = w1.numpy()\n",
    "w2 = w2.numpy()\n",
    "print(type(w1))\n",
    "\n",
    "sim = dot(matutils.unitvec(w1), matutils.unitvec(w2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get cosine similarity\n",
    "def cos_similarity(word1_embedding, word2_embedding):\n",
    "    word1_embedding = np.array(word1_embedding)\n",
    "    word2_embedding = np.array(word2_embedding)\n",
    "   \n",
    "    ans = dot(matutils.unitvec(word1_embedding), matutils.unitvec(word2_embedding))\n",
    "    return ans\n",
    "\n",
    "# Function to get Pearson correlation\n",
    "def pearson_correlation(word1_embedding, word2_embedding):\n",
    "    emb1 = np.array(word1_embedding)\n",
    "    emb2 = np.array(word2_embedding)\n",
    "\n",
    "    correlation, _ = pearsonr(emb1, emb2)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sim(df, lemmatizer, stemmer):\n",
    "    cosine_similarity_scores = []\n",
    "    pearson_correlation_scores = []\n",
    "    scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word1 = row['Word 1']\n",
    "        word2 = row['Word 2']\n",
    "        \n",
    "        # Get embeddings\n",
    "        word1_embedding = get_word_embedding(word1).squeeze()\n",
    "        word2_embedding = get_word_embedding(word2).squeeze()\n",
    "\n",
    "        # Get cosine similarity\n",
    "        cosine_similarity_scores.append(cos_similarity(word1_embedding, word2_embedding))\n",
    "        \n",
    "        # Get pearson correlation\n",
    "        pearson_correlation_scores.append(pearson_correlation(word1_embedding, word2_embedding))\n",
    "\n",
    "        # Get score\n",
    "        scores.append(row['Human (mean)'])\n",
    "        \n",
    "    return cosine_similarity_scores, pearson_correlation_scores, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity and pearson correlation scores\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "cosine_similarity_scores, pearson_correlation_scores, scores = test_sim(df, lemmatizer, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Check cosine similarity and pearson correlation scores\n",
    "print(type(cosine_similarity_scores))\n",
    "print(type(pearson_correlation_scores))\n",
    "print(type(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtcion to get spearman correlation using cosine similarity scores\n",
    "def spearman_correlation(cosine_similarity_scores, simlex_scores):\n",
    "    # Scale cosine similarity scores to 0-10\n",
    "    cosine_similarity_scores = np.array(cosine_similarity_scores)\n",
    "    cosine_similarity_scores = (1+cosine_similarity_scores)*5\n",
    "    simlex_scores = np.array(simlex_scores)\n",
    "\n",
    "    correlation, _ = spearmanr(cosine_similarity_scores, simlex_scores)\n",
    "    return correlation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Spearman correlation:  0.5433753781774955\n"
     ]
    }
   ],
   "source": [
    "# Print the initial spearman correlation\n",
    "spearman_value_sim = spearman_correlation(cosine_similarity_scores, scores)\n",
    "print(\"Initial Spearman correlation: \", spearman_value_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
